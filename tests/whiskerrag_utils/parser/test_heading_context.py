#!/usr/bin/env python3
"""
æµ‹è¯• YuqueParser çš„æ ‡é¢˜ä¸Šä¸‹æ–‡åŠŸèƒ½
"""
import asyncio
import os
import re
import sys
from typing import Dict, List, Tuple, Union

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = os.path.abspath(os.path.dirname(__file__))
sys.path.insert(0, os.path.join(project_root, "src"))


# ç®€åŒ–çš„ç±»å®šä¹‰ï¼Œé¿å…å¤æ‚çš„ä¾èµ–
class Text:
    def __init__(self, content: str, metadata: dict = None):
        self.content = content
        self.metadata = metadata or {}


class Image:
    def __init__(self, url: str, metadata: dict = None):
        self.url = url
        self.metadata = metadata or {}


class YuqueSplitConfig:
    def __init__(
        self,
        chunk_size=1000,
        chunk_overlap=200,
        separators=None,
        is_separator_regex=True,
    ):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.separators = separators
        self.is_separator_regex = is_separator_regex


# ç®€åŒ–çš„ RecursiveCharacterTextSplitter
class RecursiveCharacterTextSplitter:
    def __init__(
        self, chunk_size=1000, chunk_overlap=200, separators=None, keep_separator=False
    ):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.separators = separators or ["\n\n", "\n", " ", ""]
        self.keep_separator = keep_separator

    def split_text(self, text: str) -> List[str]:
        # ç®€åŒ–çš„åˆ†å‰²é€»è¾‘
        chunks = []
        current_chunk = ""

        for separator in self.separators:
            if separator in text:
                parts = text.split(separator)
                for i, part in enumerate(parts):
                    if len(current_chunk) + len(part) <= self.chunk_size:
                        current_chunk += part
                        if i < len(parts) - 1:
                            current_chunk += separator
                    else:
                        if current_chunk:
                            chunks.append(current_chunk)
                        current_chunk = part
                        if i < len(parts) - 1:
                            current_chunk += separator
                break

        if current_chunk:
            chunks.append(current_chunk)

        return chunks if chunks else [text]


# ç®€åŒ–çš„ YuqueParser
class YuqueParser:
    def _extract_headings(self, content: str) -> List[Tuple[int, str, int]]:
        """
        Extract headings from markdown content.
        Returns list of (level, title, position) tuples.
        """
        heading_pattern = r"^(#{1,6})\s+(.+)$"
        headings = []

        for match in re.finditer(heading_pattern, content, re.MULTILINE):
            level = len(match.group(1))  # number of # characters
            title = match.group(2).strip()
            position = match.start()
            headings.append((level, title, position))

        return headings

    def _build_heading_hierarchy(
        self, headings: List[Tuple[int, str, int]]
    ) -> Dict[int, List[str]]:
        """
        Build heading hierarchy for heading positions only.
        Returns a mapping from heading position to list of hierarchical headings.
        Optimized to store only heading positions instead of every character position.
        """
        hierarchy_map = {}
        current_hierarchy: List[Union[str, None]] = [
            None
        ] * 6  # Support up to 6 levels of headings

        for level, title, position in headings:
            # Update current hierarchy
            current_hierarchy[level - 1] = title
            # Clear deeper levels
            for j in range(level, 6):
                if j > level - 1:
                    current_hierarchy[j] = None

            # Store hierarchy only for this heading position
            active_hierarchy = [h for h in current_hierarchy if h is not None]
            hierarchy_map[position] = active_hierarchy.copy()

        return hierarchy_map

    def _find_chunk_headings(
        self,
        chunk_text: str,
        original_content: str,
        hierarchy_map: Dict[int, List[str]],
    ) -> List[str]:
        """
        Find the relevant headings for a given chunk based on its position in the original content.
        Uses binary search for efficient lookup of the closest heading position.
        """
        # Find the position of this chunk in the original content
        chunk_start = original_content.find(chunk_text)
        if chunk_start == -1:
            # If exact match not found, try to find the best match
            # This can happen due to text processing differences
            return []

        # Get sorted heading positions for binary search
        heading_positions = sorted(hierarchy_map.keys())

        if not heading_positions:
            return []

        # Find the closest heading position that is <= chunk_start
        # Using binary search for efficiency
        left, right = 0, len(heading_positions) - 1
        best_position = -1

        while left <= right:
            mid = (left + right) // 2
            if heading_positions[mid] <= chunk_start:
                best_position = heading_positions[mid]
                left = mid + 1
            else:
                right = mid - 1

        # Return the hierarchy for the best position found
        if best_position != -1:
            return hierarchy_map[best_position]
        else:
            return []

    async def parse(self, knowledge, content: Text):
        split_config = knowledge.split_config

        # Extract headings and build hierarchy
        headings = self._extract_headings(content.content)
        hierarchy_map = self._build_heading_hierarchy(headings)

        separators = split_config.separators or [
            "\n#{1,6} ",
            "```\n",
            "\n\\*\\*\\*+\n",
            "\n---+\n",
            "\n___+\n",
            "\n\n",
            "\n",
            " ",
            "",
        ]
        if "" not in separators:
            separators.append("")

        splitter = RecursiveCharacterTextSplitter(
            chunk_size=split_config.chunk_size,
            chunk_overlap=split_config.chunk_overlap,
            separators=separators,
            keep_separator=False,
        )

        result = []

        # extract all image urls and alt text
        image_pattern = r"!\[(.*?)\]\((.*?)\)"
        all_image_matches = re.findall(image_pattern, content.content)

        # create image objects
        for img_idx, (alt_text, img_url) in enumerate(all_image_matches):
            if img_url.strip():  # ensure url is not empty
                img_metadata = content.metadata.copy()
                img_metadata["_img_idx"] = img_idx
                img_metadata["_img_url"] = img_url.strip()
                img_metadata["_alt_text"] = alt_text.strip() if alt_text.strip() else ""
                image_obj = Image(url=img_url.strip(), metadata=img_metadata)
                result.append(image_obj)

        # split text
        split_texts = splitter.split_text(content.content)

        for idx, text in enumerate(split_texts):
            metadata = content.metadata.copy()
            metadata["_idx"] = idx

            # Find relevant headings for this chunk
            chunk_headings = self._find_chunk_headings(
                text, content.content, hierarchy_map
            )

            # Build context with knowledge name and headings
            context_parts = []
            if knowledge.knowledge_name:
                context_parts.append(knowledge.knowledge_name)
                metadata["_knowledge_name"] = knowledge.knowledge_name

            if chunk_headings:
                metadata["_headings"] = chunk_headings
                metadata["_heading_path"] = " > ".join(chunk_headings)
                context_parts.extend(chunk_headings)

            # Add context information to content
            if context_parts:
                full_context = " > ".join(context_parts)

                # Prepend context to the chunk content for better retrieval
                # This creates a more context-rich chunk that includes knowledge name and hierarchical path
                enhanced_content = f"[Context: {full_context}]\n\n{text}"
                result.append(Text(content=enhanced_content, metadata=metadata))
            else:
                result.append(Text(content=text, metadata=metadata))

        return result


async def test_heading_context():
    """æµ‹è¯•æ ‡é¢˜ä¸Šä¸‹æ–‡åŠŸèƒ½"""

    # æµ‹è¯•ç”¨çš„ Markdown å†…å®¹
    markdown_content = """# ğŸ§© å‘å¸ƒç‰¹æ€§ä¸€è§ˆ
## â¤ï¸ å–œæ¬¢å¹¶æ”¶è— ä¼˜ç§€UI
å…è®¸ç”¨æˆ·ç‚¹èµå¹¶æ”¶è—ç²¾å½©çš„UIç”Ÿæˆç»“æœï¼Œæ„å»ºä¸ªäººçµæ„Ÿåº“

**ã€Œæœ•å·²é˜…ï¼Œç”šæ¬¢ï¼Œèµå°”æ”¶è—ã€**


![å–œæ¬¢å¹¶æ”¶è—åŠŸèƒ½æ¼”ç¤º](https://example.com/image1.gif)

## âœï¸ éœ€æ±‚è‡ªåŠ¨æ‰©å†™
ç”¨æˆ·é¦–æ¬¡å¯¹è¯çš„éœ€æ±‚æè¿°è¿‡äºç®€å•æ—¶ï¼Œè‡ªåŠ¨æ‰©å†™ç”¨æˆ·éœ€æ±‚ï¼Œæ™ºèƒ½è¡¥å…¨è®¾è®¡æ„å›¾ï¼Œç”Ÿæˆæ›´å®Œå–„çš„UIç•Œé¢ã€‚

**ã€Œè¯»å¿ƒæœ¯MAXã€â€”â€” ç»ˆäºæœ‰AIæ‡‚æˆ‘çš„æ„æ€äº†ï¼Œè‡ªåŠ¨åŒ¹é…ä¸€ä¸ªäº§å“ç»ç†**

![éœ€æ±‚è‡ªåŠ¨æ‰©å†™åŠŸèƒ½æ¼”ç¤º](https://example.com/image2.gif)

## ğŸ¨ è®¾è®¡è§„èŒƒè‡ªåŠ¨ä»è®¾è®¡ç¨¿ä¸­æŠ½å–
ç”¨æˆ·ä¸Šä¼ äº†è®¾è®¡ç¨¿ä½†æ²¡æœ‰å¡«å†™è®¾è®¡è§„èŒƒæ—¶ï¼Œæ™ºèƒ½åˆ†æä¸Šä¼ å›¾ç‰‡ï¼Œè‡ªåŠ¨æå–å’Œåº”ç”¨è®¾è®¡è§„èŒƒï¼Œä¿æŒå“ç‰Œä¸€è‡´æ€§

**ã€Œè®¾è®¡å¸ˆæ‰çº¿æ£€æµ‹å™¨ã€â€”â€” ä¸€é”®ç ´è§£è®¾è®¡å¸ˆæš—è—çš„ç„æœºï¼Œè¿Sketchéƒ½ç¾¡æ…•çš„æŠ½å–æŠ€èƒ½ã€‚**

![è®¾è®¡è§„èŒƒæŠ½å–åŠŸèƒ½æ¼”ç¤º](https://example.com/image3.gif)

### å­åŠŸèƒ½è¯¦è§£
è¿™é‡Œæ˜¯å­åŠŸèƒ½çš„è¯¦ç»†è¯´æ˜å†…å®¹ã€‚

## è‡ªåŠ¨å–å
åœ¨ç”ŸæˆUIè¿‡ç¨‹ä¸­ï¼Œæ™ºèƒ½ä¸ºUIå’Œå¯¹è¯åœºæ™¯èµ‹äºˆæ°å½“ä¸”æœ‰æ„ä¹‰ï¼ˆå§è™AI å¾ˆè°ƒçš®ï¼‰çš„åç§°ã€‚

**ã€Œå‘Šåˆ«å‘½åå±æœºã€â€”â€” æ¯”ç¨‹åºå‘˜èµ·å˜é‡åé è°±å¤šäº†**

![](https://example.com/image4.png)
"""

    # åˆ›å»ºä¸€ä¸ªç®€å•çš„ mock Knowledge å¯¹è±¡
    class MockKnowledge:
        def __init__(self):
            self.knowledge_name = "UIè®¾è®¡ç‰¹æ€§æ–‡æ¡£"  # æ·»åŠ çŸ¥è¯†åç§°
            self.split_config = YuqueSplitConfig(
                chunk_size=500,  # è¾ƒå°çš„chunk_sizeä»¥ä¾¿æµ‹è¯•
                chunk_overlap=100,
                separators=[
                    "\n#{1,6} ",
                    "```\n",
                    "\n\\*\\*\\*+\n",
                    "\n---+\n",
                    "\n___+\n",
                    "\n\n",
                    "\n",
                    " ",
                    "",
                ],
                is_separator_regex=True,
            )

    knowledge = MockKnowledge()

    # åˆ›å»ºæµ‹è¯•ç”¨çš„ Text å¯¹è±¡
    text_content = Text(
        content=markdown_content, metadata={"source": "test", "title": "å‘å¸ƒç‰¹æ€§ä¸€è§ˆ"}
    )

    # åˆ›å»ºè§£æå™¨å®ä¾‹
    parser = YuqueParser()

    # æ‰§è¡Œè§£æ
    result = await parser.parse(knowledge, text_content)

    # åˆ†æç»“æœ
    text_objects = [item for item in result if hasattr(item, "content")]
    image_objects = [item for item in result if hasattr(item, "url")]

    print(f"è§£æç»“æœç»Ÿè®¡:")
    print(f"- æ–‡æœ¬å—æ•°é‡: {len(text_objects)}")
    print(f"- å›¾ç‰‡æ•°é‡: {len(image_objects)}")
    print()

    print("æå–çš„å›¾ç‰‡:")
    for i, img in enumerate(image_objects):
        print(f"  {i+1}. URL: {img.url}")
        print(f"     æè¿°: {img.metadata.get('_alt_text', 'æ— æè¿°')}")
        print(f"     å›¾ç‰‡URL: {img.metadata.get('_img_url', 'N/A')}")
        print(f"     å›¾ç‰‡ç´¢å¼•: {img.metadata.get('_img_idx', 'N/A')}")
        print()

    for i, text_obj in enumerate(text_objects[:5]):  # æ˜¾ç¤ºå‰5ä¸ªæ–‡æœ¬å—
        print(f"  {i+1}. é•¿åº¦: {len(text_obj.content)} å­—ç¬¦")
        print(f"     å†…å®¹é¢„è§ˆ: {text_obj.content[:150]}...")

        # æ˜¾ç¤ºæ–°å¢çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
        metadata = text_obj.metadata
        if "_knowledge_name" in metadata:
            print(f"     çŸ¥è¯†åç§°: {metadata['_knowledge_name']}")
        if "_headings" in metadata:
            print(f"     æ ‡é¢˜å±‚çº§: {metadata['_headings']}")
        if "_heading_path" in metadata:
            print(f"     æ ‡é¢˜è·¯å¾„: {metadata['_heading_path']}")

        # æ£€æŸ¥å†…å®¹æ˜¯å¦åŒ…å«ä¸Šä¸‹æ–‡æ ‡è®°
        if text_obj.content.startswith("[Context:"):
            context_end = text_obj.content.find("]\n\n")
            if context_end != -1:
                context_info = text_obj.content[
                    9:context_end
                ]  # æå– [Context: å’Œ ] ä¹‹é—´çš„å†…å®¹
                print(f"     å†…å®¹ä¸­çš„ä¸Šä¸‹æ–‡: {context_info}")

        print(
            f"     å…¶ä»–å…ƒæ•°æ®: {dict((k, v) for k, v in metadata.items() if not k.startswith('_'))}"
        )
        print()

    # éªŒè¯æ ‡é¢˜æå–åŠŸèƒ½
    print("æ ‡é¢˜æå–éªŒè¯:")
    headings = parser._extract_headings(markdown_content)
    print(f"æå–åˆ°çš„æ ‡é¢˜æ•°é‡: {len(headings)}")
    for level, title, pos in headings:
        print(f"  çº§åˆ«{level}: {title} (ä½ç½®: {pos})")
    print()

    # éªŒè¯å±‚çº§æ„å»ºåŠŸèƒ½
    print("æ ‡é¢˜å±‚çº§æ„å»ºéªŒè¯:")
    hierarchy_map = parser._build_heading_hierarchy(headings)
    print(f"å±‚çº§æ˜ å°„æ¡ç›®æ•°: {len(hierarchy_map)}")
    # æ˜¾ç¤ºå‡ ä¸ªå…³é”®ä½ç½®çš„å±‚çº§ä¿¡æ¯
    sample_positions = sorted(hierarchy_map.keys())[:5]
    for pos in sample_positions:
        print(f"  ä½ç½® {pos}: {hierarchy_map[pos]}")
    print()


if __name__ == "__main__":
    asyncio.run(test_heading_context())
